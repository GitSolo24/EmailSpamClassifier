{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f679e115-0ab5-4ecb-896f-f4c19dfb81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/solougbane/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/solougbane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/solougbane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries \n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn as scikit_learn\n",
    "import re \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "52a50937-bb5a-45ae-8731-94866c0d2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcleaner(text):\n",
    "    #Remove special characters, punctuations and numbers \n",
    "    text = re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    \n",
    "    # turn text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # lemmatize test \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemwords = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    #Join tokens back in single string \n",
    "    processed_text = ' '.join(lemwords)\n",
    "    return processed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7edc88e5-c9a1-4323-a4cd-696d617840bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write spam training files into training dataframe \n",
    "spamtrain_directory = '/Users/solougbane/Desktop/Project_1/emails/spamtraining'\n",
    "\n",
    "spamdata = []\n",
    "\n",
    "for filename in os.listdir(spamtrain_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        # read file contents \n",
    "        with open(os.path.join(spamtrain_directory, filename), 'r',errors = 'ignore') as file:\n",
    "            spamcontent = file.read()\n",
    "            #Pre-process the text \n",
    "            processed_text = wordcleaner(spamcontent)\n",
    "            # add processed text to list \n",
    "            spamdata.append({'content': processed_text, 'spam' : 1})\n",
    "            \n",
    "#Write ham training files into training dataframe\n",
    "hamtrain_directory = '/Users/solougbane/Desktop/Project_1/emails/hamtraining'\n",
    "for filename in os.listdir(hamtrain_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        # read file contents \n",
    "        with open(os.path.join(hamtrain_directory, filename), 'r') as file:\n",
    "            hamcontent = file.read()\n",
    "            #Pre-process the ham text \n",
    "            processed_text = wordcleaner(hamcontent)\n",
    "            #add processed ham text to list \n",
    "            spamdata.append({'content': processed_text, 'spam' : 0})\n",
    "\n",
    "train_df = pd.DataFrame(spamdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "54553995-c224-4376-a581-fc6419601ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject', '', 'communicating', 'effectively', 'course', 'offering', 'daren', '', 'effective', 'communication', 'course', 'december', '7', '', 'thursday', '', '1', '', '5', 'pm', '', 'one', 'suggested', '', '', 'may', 'attend', 'one', '', 'please', 'let', 'know', '', 'listed', 'ernie', '', '200', '', 'thanks', 'looking', '']\n"
     ]
    }
   ],
   "source": [
    "print(content_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "376e0c02-2b0c-4770-b588-5340be4e24b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text: Subject: communicating effectively course offering\n",
      "daren -\n",
      "there is an effective communication course on december 7 ( thursday ) from\n",
      "1 - 5 pm . is this the one you suggested for me , and if it is , may i attend this\n",
      "one ? please let me know . it is listed through ernie and is $ 200 . thanks\n",
      "for looking into this .\n"
     ]
    }
   ],
   "source": [
    "print(\"raw text:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8560a8d7-b070-4695-9ce8-6a6ce4de429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered text: subject communicating effectively course offering daren effective communication course december thursday pm one suggested may attend one please let know listed ernie thanks looking\n"
     ]
    }
   ],
   "source": [
    "print(\"filtered text:\", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dd65c6df-684c-4804-8e08-e252328c2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  200 non-null    object\n",
      " 1   spam     200 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fc1d6318-09c7-476e-afc1-190f3bfa1c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject quick way buy soft ware variety top ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject heisser fetish mann war da ein wochene...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject nb real vallum x anax l evitra soma mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject work wondder dear sir madam please pol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject get free ibm thinkpad computer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  spam\n",
       "0  subject quick way buy soft ware variety top ma...     1\n",
       "1  subject heisser fetish mann war da ein wochene...     1\n",
       "2  subject nb real vallum x anax l evitra soma mu...     1\n",
       "3  subject work wondder dear sir madam please pol...     1\n",
       "4             subject get free ibm thinkpad computer     1"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "61d5576d-ca2d-46a7-8242-18b41ac84009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>subject neon verse see tonight love scripture doc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>subject fw king ranch balancing xl saxet flowi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>subject hpl delivery meter cheryl documentatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>subject devon availability may forwarded victo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>subject communicating effectively course offer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  spam\n",
       "195  subject neon verse see tonight love scripture doc     0\n",
       "196  subject fw king ranch balancing xl saxet flowi...     0\n",
       "197  subject hpl delivery meter cheryl documentatio...     0\n",
       "198  subject devon availability may forwarded victo...     0\n",
       "199  subject communicating effectively course offer...     0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b59df5e9-e829-403e-895d-91aa1c19d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn training data (words/content) into vectors \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "train_cv = cv.fit_transform(train_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a1f9b43d-26f4-44f8-973f-1eed83190018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5946)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f791b863-1603-4d4d-8b75-b196f7d1f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign X and Y training data\n",
    "x_train = train_cv\n",
    "train_label = train_df['spam']\n",
    "y_train = train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b31028d3-b411-48aa-b5ef-2b911b46128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import testing data \n",
    "spamtest_directory = '/Users/solougbane/Desktop/Project_1/emails/spamtesting'\n",
    "\n",
    "spamtest = []\n",
    "\n",
    "for filename in os.listdir(spamtest_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        # read spam testing files \n",
    "        with open(os.path.join(spamtest_directory, filename), 'r',errors = 'ignore') as file:\n",
    "            spamtest_content = file.read()\n",
    "            # pre-process the spam testing data\n",
    "            processed_text = wordcleaner(spamtest_content)\n",
    "            # add spam testing to list \n",
    "            spamtest.append({'content': processed_text, 'spam' : 1})\n",
    "\n",
    "hamtest_directory = '/Users/solougbane/Desktop/Project_1/emails/hamtesting'\n",
    "for filename in os.listdir(hamtest_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        # read ham testing files \n",
    "        with open(os.path.join(hamtest_directory, filename), 'r') as file:\n",
    "            hamtest_content = file.read()\n",
    "            # pre-processing the ham testing data\n",
    "            processed_text = wordcleaner(hamtest_content)\n",
    "            # add ham testing data to list \n",
    "            spamtest.append({'content': processed_text, 'spam' : 0})\n",
    "\n",
    "test_df = pd.DataFrame(spamtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5f64d27d-6820-4edf-abd0-0517993fbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  200 non-null    object\n",
      " 1   spam     200 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4f6c6db9-ff95-4a2b-a1a7-3ed3d4401c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn testing data into vectors\n",
    "test_cv = cv.transform(test_df['content'])\n",
    "x_test = test_cv\n",
    "testlabel = test_df['spam']\n",
    "y_test = testlabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406afb56-0482-4823-8b6a-a80761b4f841",
   "metadata": {},
   "source": [
    "Applying SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2e2f9386-173d-4645-9335-cad5031f4250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import SVC module \n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "svm_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f42d2d-b321-4181-9de0-80fc8ff52dea",
   "metadata": {},
   "source": [
    "Evaluate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5d04263f-4fe4-49e5-bd76-c07be4fa66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74  26]\n",
      " [  0 100]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "conmat = confusion_matrix(y_train,y_train_predict)\n",
    "print(conmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f770fece-00a5-4a48-bcb1-6b1a99ff1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.61       100\n",
      "           1       0.64      1.00      0.78       100\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.82      0.72      0.70       200\n",
      "weighted avg       0.82      0.72      0.70       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "y_train_predict = svm_classifier.predict(x_train)\n",
    "#predict labels \n",
    "y_test_predict = svm_classifier.predict(x_test)\n",
    "print(classification_report(y_test,y_test_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
